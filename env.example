# BookSpine Environment Configuration
# Copy this file to .env and fill in your values

# --- Hugging Face Configuration ---

# Hugging Face API Token (Optional but Recommended)
# Get a free token at: https://huggingface.co/settings/tokens
# This helps avoid rate limiting when downloading models from the Hugging Face Hub.
HF_TOKEN=your_huggingface_token_here

# Hugging Face Cache Directory (Optional)
# Specifies the directory to cache downloaded models and datasets.
# Default: ~/.cache/huggingface
HF_HOME=~/.cache/huggingface

# Disable Hugging Face Telemetry (Recommended)
# Opt-out of sending telemetry data to Hugging Face.
HF_HUB_DISABLE_TELEMETRY=1

# Disable Implicit Token (Recommended)
# Prevents the Hugging Face library from using a token found in the cache.
HF_HUB_DISABLE_IMPLICIT_TOKEN=1


# --- Keyword Theme Extraction (KTE) Configuration ---

# The name of the sentence-transformer model to use for keyword extraction.
# This model will be downloaded from the Hugging Face Hub.
KTE_MODEL_NAME=sentence-transformers/all-MiniLM-L6-v2

# The inference engine to use for keyword extraction.
# Valid options are:
#   - 'local': Run the model on your local machine using sentence-transformers. (Default)
#   - 'hf': Use the Hugging Face Inference API. Requires HF_TOKEN.
#   - 'stapi': Use a self-hosted sentence-transformers API.
#   - 'infinity': Use the Infinity API (https://github.com/michaelfeil/infinity). (DEPRECATED)
KTE_ENGINE=local

# The authentication token for the inference API (e.g., Hugging Face API token).
KTE_AUTH_TOKEN=${HF_TOKEN}

# The URL of the inference API.
# This should be set to the appropriate endpoint for the selected KTE_ENGINE.
# Example for Hugging Face: https://api-inference.huggingface.co/pipeline/feature-extraction/sentence-transformers/all-MiniLM-L6-v2
# Example for STAPI: http://localhost:8000/v1/embeddings
KTE_API_URL=https://api-inference.huggingface.co/pipeline/feature-extraction/sentence-transformers/all-MiniLM-L6-v2
